{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import multiprocessing as mp\n",
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize text files downloads\n",
    "### Exercise 1\n",
    "Use `ThreadPoolExecutor` to parallelize the text downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir books\n",
    "\n",
    "import urllib.request as url\n",
    "\n",
    "source = \"https://mmassd.github.io/\"\n",
    "text = [\n",
    "    \"books/hugo.txt\",\n",
    "    \"books/proust.txt\",\n",
    "    \"books/zola.txt\",\n",
    "    \"books/stendhal.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "download = lambda t: url.urlretrieve(source+t, filename=t)\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    pool.map(download, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ETUDES\\M1 MAS\\M_Python\\S3\\BigData\\books\n",
      "D:\\ETUDES\\M1 MAS\\M_Python\\S3\\BigData\n"
     ]
    }
   ],
   "source": [
    "%cd books\n",
    "!del *.txt\n",
    "%cd ..\n",
    "%rmdir books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel map\n",
    "### Exercise 2\n",
    "Modify the Map Reduce's `mapper` function by adding process name print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(path):\n",
    "    print(mp.current_process().name)\n",
    "    with open(path, 'r') as f:\n",
    "        return [(word, 1)\n",
    "                for word in sorted(f.read().replace(\".\", \" \").lower().split())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel reduce\n",
    "### Exercise 3\n",
    "Write a parallel program that uses the three functions above using `ProcessPoolExecutor`. It reads all the “sample*.txt” files. Map and reduce steps are parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorem import text\n",
    "for i in range(8):\n",
    "    with open(\"sample{0:02d}.txt\".format(i), \"w\") as f:\n",
    "        f.write(text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing iter.py\n"
     ]
    }
   ],
   "source": [
    "%%file iter.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def mapper(path):\n",
    "    print(mp.current_process().name)\n",
    "    with open(path, 'r') as f:\n",
    "        return [(word, 1)\n",
    "                for word in sorted(f.read().replace(\".\", \" \").lower().split())]\n",
    "\n",
    "def partitioner(pairs):\n",
    "    partition = dict()\n",
    "    for key, val in pairs:\n",
    "        partition[key] = partition.get(key, []) + [val]\n",
    "    return partition\n",
    "\n",
    "def reducer(tuplew):\n",
    "    return tuplew[0], sum(tuplew[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(glob.glob('sample0*.txt'))\n",
    "    with ProcessPoolExecutor() as e:\n",
    "        count = e.map(\n",
    "            reducer,\n",
    "            partitioner(itertools.chain(*e.map(\n",
    "                mapper, files\n",
    "            ))).items()\n",
    "        )\n",
    "    print(list(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpawnProcess-7\n",
      "SpawnProcess-1\n",
      "SpawnProcess-2\n",
      "SpawnProcess-2\n",
      "SpawnProcess-3\n",
      "SpawnProcess-4\n",
      "SpawnProcess-4\n",
      "SpawnProcess-5\n",
      "[('adipisci', 55), ('aliquam', 47), ('amet', 54), ('consectetur', 51), ('dolor', 54), ('dolore', 42), ('dolorem', 40), ('eius', 53), ('est', 55), ('etincidunt', 51), ('ipsum', 54), ('labore', 53), ('magnam', 46), ('modi', 49), ('neque', 49), ('non', 47), ('numquam', 54), ('porro', 51), ('quaerat', 46), ('quiquia', 56), ('quisquam', 58), ('sed', 45), ('sit', 43), ('tempora', 45), ('ut', 53), ('velit', 52), ('voluptatem', 46)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} iter.py\n",
    "!del -r *.txt\n",
    "!del iter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase volume of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting content 100 of 100\r"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import *\n",
    "\n",
    "base_url = \"http://www.thelatinlibrary.com/\"\n",
    "home_content = urlopen(base_url)\n",
    "\n",
    "soup = BeautifulSoup(home_content, \"lxml\")\n",
    "author_page_links = soup.find_all(\"a\")\n",
    "author_pages = [ap[\"href\"] for i, ap in enumerate(author_page_links) if i < 49]\n",
    "\n",
    "ap_content = list()\n",
    "for ap in author_pages:\n",
    "    try: ap_content.append(urlopen(base_url + ap))\n",
    "    except: continue\n",
    "\n",
    "book_links = list()\n",
    "for path, content in zip(author_pages, ap_content):\n",
    "    author_name = path.split(\".\")[0]\n",
    "    ap_soup = BeautifulSoup(content, \"lxml\")\n",
    "    book_links += ([link for link in ap_soup.find_all(\"a\", {\"href\": True}) if author_name in link[\"href\"]])\n",
    "\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "num_pages = 100\n",
    "\n",
    "for i, bl in enumerate(book_links[:num_pages]):\n",
    "    print(\"Getting content \" + str(i + 1) + \" of \" + str(num_pages), end=\"\\r\", flush=True)\n",
    "    try:\n",
    "        content = urlopen(base_url + bl[\"href\"]).read()\n",
    "        with open(f\"book-{i:03d}.dat\",\"wb\") as f:\n",
    "            f.write(content)\n",
    "    except HTTPError as err:\n",
    "        print(\"Unable to retrieve \" + bl[\"href\"] + \".\")\n",
    "        continue\n",
    "\n",
    "from glob import glob\n",
    "files = glob('book*.dat')\n",
    "texts = list()\n",
    "for file in files:\n",
    "    with open(file,'rb') as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the text from html and split the text at periods to convert it into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post emensos insuperabilis expeditionis eventus languentibus partium animis quas periculorum varietas fregerat et laborum nondum tubarum cessante clangore vel milite locato per stationes hibernas fortunae saevientis procellae tempestates alias rebus infudere communibus per multa illa et dira facinora caesaris galli qui ex squalore imo miseriarum in aetatis adultae primitiis ad principale culmen insperato saltu provectus ultra terminos potestatis delatae procurrens asperitate nimia cuncta foedabat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(\"Document \" + str(i + 1) + \" of \" + str(len(texts)), end=\"\\r\", flush=True)\n",
    "    textSoup = BeautifulSoup(text, \"lxml\")\n",
    "    paragraphs = textSoup.find_all(\"p\", attrs={\"class\":None})\n",
    "    prepared = (\"\".join([p.text.strip().lower() for p in paragraphs[1:-1]]))\n",
    "    for t in prepared.split(\".\"):\n",
    "        part = \"\".join([c for c in t if c.isalpha() or c.isspace()])\n",
    "        sentences.append(part.strip())\n",
    "\n",
    "# print first and last sentence to check the results\n",
    "print(sentences[0])\n",
    "print(sentences[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Parallelize this last process using `concurrent.futures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post emensos insuperabilis expeditionis eventus languentibus partium animis quas periculorum varietas fregerat et laborum nondum tubarum cessante clangore vel milite locato per stationes hibernas fortunae saevientis procellae tempestates alias rebus infudere communibus per multa illa et dira facinora caesaris galli qui ex squalore imo miseriarum in aetatis adultae primitiis ad principale culmen insperato saltu provectus ultra terminos potestatis delatae procurrens asperitate nimia cuncta foedabat\n"
     ]
    }
   ],
   "source": [
    "lower = lambda p: p.text.strip().lower()\n",
    "sent = lambda l: (\"\".join(filter(lambda c: c.isalpha() or c.isspace(), l))).strip()\n",
    "\n",
    "\n",
    "def sent_tokenizer(text):\n",
    "    paragraphs = BeautifulSoup(text, \"lxml\").find_all(\"p\",attrs={\"class\": None})\n",
    "    return map(sent, (\"\".join(map(lower, paragraphs[1:-1]))).split(\".\"))\n",
    "\n",
    "with ThreadPoolExecutor() as t:\n",
    "    f = t.map(sent_tokenizer, texts)\n",
    "    sentences_thread = list(itertools.chain(*f))\n",
    "\n",
    "print(sentences_thread[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!del *.dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
